#!/usr/bin/env python3
'''Convert a PSQL generate CSV file into a set of NginX redirect_map.conf after migration 
using eprints_to_rdm.py.
'''

#
# Make NginX friendly redirects for migrated records in log
# from eprints_to_rdm.py.
#
import sys
import os
import csv
from datetime import datetime
from subprocess import Popen, PIPE

def check_environment(app_name):
    '''Check to make sure all the environment variables have values and are avia
lable'''
    varnames = [
        # We use the REPO_ID, e.g. caltechauthors to know which database to access with psql
        'REPO_ID'
    ]
    config = { 'APP_NAME': app_name}
    is_ok = True
    for varname in varnames:
        val = os.getenv(varname, None)
        if val is None:
            print('REPO_ID not set in enviroment', file = sys.stderr)
            is_ok = False
        else:
            config[varname] = val
    return config, is_ok

def generate_csv_file(config, csv_name):
    '''generate a CSV file for deriving redirects from psql database. Requires REPO_ID to be set in
the environment.'''
    repo_id = config.get('REPO_ID', None)
    app_name = config.get('APP_NAME', os.path.basename(sys.argv[0]))
    start = datetime.now().isoformat(timespec='seconds') 
    print(f'generating {csv_name} from {repo_id} started {start}', file = sys.stderr)
    if repo_id is None:
        print('Cannot generate CSV file, REPO_ID is not found in environent', file = sys.stderr)
        sys.exit(1)
    sql_file = "generate_redirect_csv.sql"
    if not os.path.exists(sql_file):
        print(f'creating {sql_file}', sys.stderr)
        with open(sql_file, 'w', encoding = 'utf-8') as _f:
            _f.write(f'''--
-- filename: {sql_file}
-- generated by {app_name} on {start}
--
-- This SQL file generates a CSV file suitable for mapping redirects for imported EPrint ids.
-- The resulting file is is written to standard out in CSV format.
-- 
COPY (
	SELECT (t1.identifiers ->> 'identifier')::DECIMAL AS eprintid, t1.rdmid AS rdmid, t1.record_status AS record_status
	FROM (SELECT json ->> 'id' AS rdmid, json -> 'access' ->> 'record' AS record_status, 
       		jsonb_array_elements(json -> 'metadata' -> 'identifiers') AS identifiers
		FROM rdm_records_metadata
		) AS t1 
	WHERE (t1.identifiers ->> 'scheme' LIKE 'eprintid')
	AND (t1.record_status LIKE 'public') 
	ORDER BY (t1.identifiers ->> 'identifier')::DECIMAL
) 
TO STDOUT (FORMAT CSV, HEADER);
''')
    cmd = [
        "psql",
        repo_id,
        "-f",
        sql_file
    ]
    with Popen(cmd, stdout = PIPE, stderr = PIPE) as proc:
        src, err = proc.communicate()
        exit_code = proc.returncode
        if exit_code > 0:
            print(err, file = sys.stderr)
            sys.exit(exit_code)
        if not isinstance(src, bytes):
            src = src.encode('utf-8')
        with open(csv_name, 'w', encoding = 'utf-8') as _f:
            _f.write(src.decode('utf-8'))
        now = datetime.now().isoformat(timespec='seconds') 
        print(f'generating {csv_name} from {repo_id} started {start}, completed {now}', file = sys.stderr)

def process_csv(config, csv_name):
    '''Read the eprints_to_rdm.py log and generate a rewrite
    rule for each entry containing migrated'''
    repo_id = config.get('REPO_ID', '')
    start = datetime.now().isoformat(timespec='seconds')   
    print(f'processing {csv_name} from {repo_id} started {start}', file = sys.stderr)
    field_names = [ 'eprintid', 'rdmid', 'record_status' ]
    with open(csv_name, newline='', encoding ='utf-8') as csvfile:
        reader = csv.DictReader(csvfile, fieldnames = field_names , restval = '')
        # Build our map of eprintid to rdmid. We're favoring the
        # the first encountered.
        print('''
map_hash_max_size 26214;
map_hash_bucket_size 26214;
map $request_uri $redirect_uri {
    # Site specific redirects.
    ~^/5456/1/hrst.mit.edu(?<suffix>.*)$ https://wayback.archive-it.org/9060/20230418124557/https://authors.library.caltech.edu/5456/1/hrst.mit.edu$suffix;

    # EPrint Records and their files redirects.
''')
        eprint_id_list = {}
        for row in reader:
            eprintid = row.get('eprintid', '').strip()
            rdmid = row.get('rdmid', '').strip()
            status = row.get('record_status', '').strip()
            if status == 'public':
                if eprintid in eprint_id_list:
                    eprint_id_list[eprintid] += 1
                    dups = eprint_id_list[eprintid]
                    print(f'#    /{eprintid}      /records/{rdmid}/latest; # {dups} found')
                    print(f'#    ~^(/{eprintid}/[0-9]*/)(?<suffix>.*)   /records/{rdmid}/files/$suffix;')
                else:
                    eprint_id_list[eprintid] = 1
                    print(f'    /{eprintid}      /records/{rdmid}/latest;')
                    print(f'    ~^(/{eprintid}/[0-9]*/)(?<suffix>.*)   /records/{rdmid}/files/$suffix;')
        print('}')
    now = datetime.now().isoformat(timespec='seconds') 
    print(f'processing {csv_name} from {repo_id} started {start}, completed {now}', file = sys.stderr)

def main():
    '''Main processing'''
    app_name = os.path.basename(sys.argv[0])
    if len(sys.argv) != 2:
        print(f'''
{app_name} requires a csv filename. CSV will be generated using psql if it does not exist.''',
file = sys.stderr)
        sys.exit(1)
    config, is_ok = check_environment(app_name)
    if not is_ok:
        sys.exit(1)
    if not os.path.exists(sys.argv[1]):
        generate_csv_file(config, sys.argv[1])
    process_csv(config, sys.argv[1])

if __name__ == '__main__':
    main()
